# Language Models

- [LLM / transformer slides](https://docs.google.com/presentation/d/15P8hdIDF4Dp_CudvfV3QYYsF7uFNsbVIDxA1ttiZR1g/edit?usp=sharing)

## Markov Chains

- ğŸ’» [Weighted Selection p5.js example](https://editor.p5js.org/a2zitp/sketches/un8B-P4jg) (with temperature!) along with ğŸš‚ [Weighted Selection Algorithm video](https://youtu.be/ETphJASzYes)
- ğŸ“š [How to Generate Text Hugging Face blog post](https://huggingface.co/blog/how-to-generate) (covers temperature, top_p, and top_k)
- ğŸ“• [Markov Chains](http://setosa.io/blog/2014/07/26/markov-chains/) by Victor Powell and Lewis Lehe

## Sequential Data and Recurrent Neural Networks

- ğŸ“š [The Unreasonable Effectiveness of RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- ğŸ¨ [Four Experiments in Handwriting with a Neural Network](https://distill.pub/2016/handwriting/) (Drawing)
- ğŸ“– [10 things artificial intelligence did in 2018](http://aiweirdness.com/post/181621835642/10-things-artificial-intelligence-did-in-2018) by Janelle Shane (Text)
- ğŸ“– [Writing with the Machine](https://www.robinsloan.com/notes/writing-with-the-machine/)

## Transformers and Large Language Models

> among the reasons I use large pre-trained language models sparingly in my computer-generated poetry practice is that being able to know whose voices I'm speaking with is... actually important, as is being understanding how the output came to have its shape - [@aparrish](https://twitter.com/aparrish/), [full thread](https://twitter.com/aparrish/status/1286808606466244608)

- ğŸ“š [Watch an A.I. Learn to Write by Reading Nothing but **\_\_\_\_**](https://www.nytimes.com/interactive/2023/04/26/upshot/gpt-from-scratch.html) by Aatish Bhatia
- ğŸ“š [Attention is All You Need](https://arxiv.org/abs/1706.03762) - Original "Transformer" paper from 2017, also [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) -- Attention paper from 2014
- ğŸ“š [What Are Transformer Models and How Do They Work?](https://docs.cohere.com/docs/transformer-models)
- ğŸ¥ [How large language models work, a visual intro to transformers](https://youtu.be/wjZofJX0v4M) by 3Blue1Brown
- ğŸ¥ [Intro to Large Language Models](https://youtu.be/zjkBMFhNj_g) by Andrej Karpathy and [Intro to LLMs slides](https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view)
- ğŸ“– [Language Models Can Only Write Ransom Notes](https://posts.decontextualize.com/language-models-ransom-notes/) by Allison Parrish

## LLM Training

- ğŸ¦™ [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
- ğŸ¦™ [The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783)
- ğŸ¦œ [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ğŸ¦œ](https://dl.acm.org/doi/10.1145/3442188.3445922)
- ğŸ” [The Foundation Model Transparency Index](https://crfm.stanford.edu/fmti/May-2024/index.html)
- ğŸ“– [Generative AIâ€™s Illusory Case for Fair Use](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4924997) by Jacqueline Charlesworth

### Datasets for LLMs

- ğŸ”¢ [Common Crawl](https://commoncrawl.org/)
- ğŸ”¢ [The Pile](https://pile.eleuther.ai/)

### Climate Impact

- ğŸŒ [The Internetâ€™s Next Great Power Suck](https://www.theatlantic.com/technology/archive/2023/08/ai-carbon-emissions-data-centers/675094/)
- âš¡ï¸ [Carbon Emissions and Large Neural Network Training ](https://arxiv.org/ftp/arxiv/papers/2104/2104.10350.pdf)

## Code Examples and Implementations

### Markov chains

- ğŸš‚ [Markov Chain Coding Challenge video + code example](https://thecodingtrain.com/challenges/42-markov-chain-name-generator)
- ğŸ’» [Markov Chain p5.js code examples](https://editor.p5js.org/a2zitp/collections/WEXEPRHuE)

### Ollama

_These examples require working with p5.js locally on your computer and outside of the web editor. Some resources for doing so can be found in [my workflow video series](https://thecodingtrain.com/tracks/2018-workflow)._

- ğŸ¦™ [Ollama: Run LLMs locally](https://ollama.ai/)
- [Ollama chat completion API](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion)
- ğŸ’» [In class Ollama test](ollama)
- ğŸ’» [Ollama with JavaScript](https://github.com/Programming-from-A-to-Z/Ollama-Examples)

### Cloud-Based APIs

- [ChatBot with Llama hosted on Replicate](https://github.com/Programming-from-A-to-Z/llama-chatbot-replicate)
- [ChatGPT Clone with OpenAI API](https://github.com/Programming-from-A-to-Z/ChatGPT-clone)

### Transformers.js

- [Chatbot with Conversation History](https://editor.p5js.org/ml_4_cc/sketches/9p1iIuauz)
- [Text Completion](https://editor.p5js.org/ml_4_cc/sketches/hTbzuq37iJ)
- [Reasoning Models](https://editor.p5js.org/xenova/sketches/DTztSM0uI)
